{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = \"TED_data/train.en\"\n",
    "train2 = \"TED_data/train.zh\"\n",
    "eval1 = \"TED_data/valid.en\"\n",
    "eval2 = \"TED_data/valid.zh\"\n",
    "test1 = \"TED_data/test.en\"\n",
    "test2 = \"TED_data/test.zh\"\n",
    "\n",
    "_prepro = lambda x:  [line.strip() for line in open(x, 'r').read().split(\"\\n\")]\n",
    "prepro_train1, prepro_train2 = _prepro(train1), _prepro(train2)\n",
    "prepro_eval1, prepro_eval2 = _prepro(eval1), _prepro(eval2)\n",
    "prepro_test1, prepro_test2 = _prepro(test1), _prepro(test2)\n",
    "\n",
    "print (prepro_train1[0])\n",
    "print (prepro_train2[0])\n",
    "print (prepro_eval1[0])\n",
    "print (prepro_eval2[0])\n",
    "print (prepro_test1[0])\n",
    "print (prepro_test2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将预处理的数据写入文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"TED_data/prepro\", exist_ok=True)\n",
    "\n",
    "def _write(sents, fname):\n",
    "    with open(fname, 'w') as fout:\n",
    "        fout.write(\"\\n\".join(sents))\n",
    "\n",
    "_write(prepro_train1, \"TED_data/prepro/train.en\")\n",
    "_write(prepro_train2, \"TED_data/prepro/train.zh\")\n",
    "_write(prepro_train1+prepro_train2, \"TED_data/prepro/train\")\n",
    "_write(prepro_eval1, \"TED_data/prepro/eval.en\")\n",
    "_write(prepro_eval2, \"TED_data/prepro/eval.zh\")\n",
    "_write(prepro_test1, \"TED_data/prepro/test.en\")\n",
    "_write(prepro_test2, \"TED_data/prepro/test.zh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "vocab_size = 32000\n",
    "\n",
    "os.makedirs(\"TED_data/segmented\", exist_ok=True)\n",
    "train = '--input=TED_data/prepro/train --pad_id=0 --unk_id=1 \\\n",
    "    --bos_id=2 --eos_id=3 --model_prefix=TED_data/segmented/bpe \\\n",
    "    --vocab_size={} --model_type=bpe'.format(vocab_size)\n",
    "spm.SentencePieceTrainer.Train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用BPE进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"TED_data/segmented/bpe.model\")\n",
    "\n",
    "def _segment_and_write(sents, fname):\n",
    "        with open(fname, \"w\") as fout:\n",
    "            for sent in sents:\n",
    "                pieces = sp.EncodeAsPieces(sent)\n",
    "                fout.write(\" \".join(pieces) + \"\\n\")\n",
    "                \n",
    "_segment_and_write(prepro_train1, \"TED_data/segmented/train.en.bpe\")\n",
    "_segment_and_write(prepro_train2, \"TED_data/segmented/train.zh.bpe\")\n",
    "_segment_and_write(prepro_eval1, \"TED_data/segmented/eval.en.bpe\")\n",
    "_segment_and_write(prepro_eval2, \"TED_data/segmented/eval.zh.bpe\")\n",
    "_segment_and_write(prepro_test1, \"TED_data/segmented/test.en.bpe\")          \n",
    "\n",
    "print(\"train1:\", open(\"TED_data/segmented/train.en.bpe\",'r').readline())\n",
    "print(\"train2:\", open(\"TED_data/segmented/train.zh.bpe\", 'r').readline())\n",
    "print(\"eval1:\", open(\"TED_data/segmented/eval.en.bpe\", 'r').readline())\n",
    "print(\"eval2:\", open(\"TED_data/segmented/eval.zh.bpe\", 'r').readline())\n",
    "print(\"test1:\", open(\"TED_data/segmented/test.en.bpe\", 'r').readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型\n",
    "\n",
    "导入词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fpath = 'TED_data/segmented/bpe.vocab' #词表文件\n",
    "vocab = [line.split()[0] for line in open(vocab_fpath, 'r').read().splitlines()]\n",
    "token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "idx2token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(list(token2idx.items())[:5])\n",
    "print(list(idx2token.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建训练和测试用的数据，以iteration形式保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen1 = 100 # 源语言句子最大长度\n",
    "maxlen2 = 100 # 目标语言句子最大长度\n",
    "batch_size = 64\n",
    "\n",
    "# 导入训练数据，过滤掉长度不符合要求的\n",
    "train_sents1, train_sents2 = [], []\n",
    "with open(\"TED_data/prepro/train.en\", 'r') as f1, open(\"TED_data/prepro/train.zh\", 'r') as f2:\n",
    "    for sent1, sent2 in zip(f1, f2):\n",
    "        if len(sent1.split()) + 1 > maxlen1: continue # 1: </s>\n",
    "        if len(sent2.split()) + 1 > maxlen2: continue  # 1: </s>\n",
    "        train_sents1.append(sent1.strip())\n",
    "        train_sents2.append(sent2.strip())\n",
    "    \n",
    "eval_sents1, eval_sents2 = [], []\n",
    "with open(\"TED_data/prepro/eval.en\", 'r') as f1, open(\"TED_data/prepro/eval.zh\", 'r') as f2:\n",
    "    for sent1, sent2 in zip(f1, f2):\n",
    "        if len(sent1.split()) + 1 > maxlen1: continue # 1: </s>\n",
    "        if len(sent2.split()) + 1 > maxlen2: continue  # 1: </s>\n",
    "        eval_sents1.append(sent1.strip())\n",
    "        eval_sents2.append(sent2.strip())\n",
    "        \n",
    "test_sents1, test_sents2 = [], []\n",
    "with open(\"TED_data/prepro/test.en\", 'r') as f1, open(\"TED_data/prepro/test.zh\", 'r') as f2:\n",
    "    for sent1, sent2 in zip(f1, f2):\n",
    "        if len(sent1.split()) + 1 > maxlen1: continue # 1: </s>\n",
    "        if len(sent2.split()) + 1 > maxlen2: continue  # 1: </s>\n",
    "        test_sents1.append(sent1.strip())\n",
    "        test_sents2.append(sent2.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据进行batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def encode(inp, types):\n",
    "    '''\n",
    "    字符串转为数字\n",
    "    如果type为x, 表示inp是源语言字符串，会在末尾加</s>\n",
    "    如果type为y，表示inp是目标语言字符串，会在开始加<s>，结尾加</s>\n",
    "    '''\n",
    "    inp_str = inp.decode(\"utf-8\")\n",
    "    if types==\"x\": tokens = inp_str.split() + [\"</s>\"]\n",
    "    else: tokens = [\"<s>\"] + inp_str.split() + [\"</s>\"]\n",
    "\n",
    "    x = [token2idx.get(t, token2idx[\"<unk>\"]) for t in tokens]\n",
    "    return x\n",
    "\n",
    "def generator_fn(sents1, sents2):\n",
    "    '''\n",
    "    为训练产生数据\n",
    "    返回：源语言数字序列、源语言序列长度、源语言字符串本身、目标语言数字序列（不包括最后一个）、\n",
    "        目标语言数字序列（不包括第一个）、目标语言序列长度、目标语言字符串本身\n",
    "    '''\n",
    "    for sent1, sent2 in zip(sents1, sents2):\n",
    "        x = encode(sent1, \"x\")\n",
    "        y = encode(sent2, \"y\")\n",
    "        decoder_input, y = y[:-1], y[1:]\n",
    "\n",
    "        x_seqlen, y_seqlen = len(x), len(y)\n",
    "        yield (x, x_seqlen, sent1), (decoder_input, y, y_seqlen, sent2)\n",
    "\n",
    "shapes = (([None], (), ()), ([None], [None], (), ()))\n",
    "types = ((tf.int32, tf.int32, tf.string), (tf.int32, tf.int32, tf.int32, tf.string))\n",
    "paddings = ((0, 0, ''),(0, 0, 0, ''))\n",
    "\n",
    "train_batches = tf.data.Dataset.from_generator(\n",
    "        generator_fn,\n",
    "        output_shapes=shapes,\n",
    "        output_types=types,\n",
    "        args=(train_sents1, train_sents2))\n",
    "train_batches = train_batches.shuffle(128 * batch_size)\n",
    "train_batches = train_batches.repeat() #可以永久地训练\n",
    "train_batches = train_batches.padded_batch(batch_size, shapes, paddings).prefetch(1)\n",
    "num_train_batches = len(train_sents1) // batch_size + int(len(train_sents1) % batch_size != 0) #计算总的batch数目\n",
    "num_train_samples = len(train_sents1)\n",
    "\n",
    "eval_batches = tf.data.Dataset.from_generator(\n",
    "        generator_fn,\n",
    "        output_shapes=shapes,\n",
    "        output_types=types,\n",
    "        args=(eval_sents1, eval_sents2))\n",
    "eval_batches = eval_batches.repeat()\n",
    "eval_batches = eval_batches.padded_batch(batch_size, shapes, paddings).prefetch(1)\n",
    "num_eval_batches = len(eval_sents1) // batch_size + int(len(eval_sents1) % batch_size != 0) #计算总的batch数目\n",
    "num_eval_samples = len(eval_sents1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter = tf.data.Iterator.from_structure(train_batches.output_types, train_batches.output_shapes)\n",
    "xs, ys = iter.get_next()\n",
    "# 迭代器初始化\n",
    "train_init_op = iter.make_initializer(train_batches)\n",
    "eval_init_op = iter.make_initializer(eval_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型定义\n",
    "\n",
    "词向量初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "d_model = 512 # 词向量维度\n",
    "with tf.variable_scope(\"shared_weight_matrix\"):\n",
    "    embeddings = tf.get_variable('weight_mat', \n",
    "                                 dtype=tf.float32, \n",
    "                                 shape=(vocab_size, d_model),\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    # 令第一行向量为0\n",
    "    embeddings = tf.concat((tf.zeros(shape=[1, d_model]), embeddings[1:, :]), 0) # embeddings->(vocab_size, d_model)\n",
    "    \n",
    "print (embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向计算过程\n",
    "\n",
    "编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dropout_rate = 0.3\n",
    "num_blocks = 6\n",
    "num_heads = 8\n",
    "padding_num = -2 ** 32 + 1\n",
    "epsilon = 1e-8\n",
    "d_ff = 2048 # 前向网络的隐藏层\n",
    "\n",
    "def encoding(xs_input, is_training):\n",
    "    with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "        x, seqlens, sents1 = xs_input\n",
    "\n",
    "        # 词向量编码\n",
    "        # x->(batch_size, maxlen1), embeddings->(vocab_size, d_model), enc->(batch_size, maxlen1, d_model)\n",
    "        enc = tf.nn.embedding_lookup(embeddings, x)\n",
    "        enc *= d_model ** 0.5 # 求开根号，将embedding进行scale\n",
    "\n",
    "        # 位置向量编码\n",
    "        E = enc.get_shape().as_list()[-1] # 常数，实际为d_model\n",
    "        N, T = tf.shape(enc)[0], tf.shape(enc)[1] # 变量，对应batch_size, maxlen1\n",
    "        with tf.variable_scope(\"positional_encoding\", reuse=tf.AUTO_REUSE):\n",
    "            # position indices\n",
    "            # [[0,1,...,maxlen1-1],[0,1,...,maxlen1-1],...[0,1,...,maxlen1-1]]\n",
    "            position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # (N, T)即(batch_size, maxlen1)\n",
    "\n",
    "            # 先计算pos/10000^2i/d_model\n",
    "            position_enc = np.array([\n",
    "                [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n",
    "                for pos in range(maxlen1)]) # position_end->(maxlen1, d_model)\n",
    "\n",
    "            # 再计算\n",
    "            # PE(pos,2i) = sin(pos/10000^2i/d_model)\n",
    "            # PE(pos,2i+1) = cos(pos/10000^2i/d_model)\n",
    "            position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i, 即(maxlen1, d_model)的d_model里的偶数位置取sin\n",
    "            position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1, 即(maxlen1, d_model)的d_model里的奇数位置取cos\n",
    "            #因为tf.nn.embedding_lookup输入为张量，所以做一次转换\n",
    "            position_enc = tf.convert_to_tensor(position_enc, tf.float32) # (maxlen1, d_model)\n",
    "\n",
    "            # lookup\n",
    "            position_embeddings = tf.nn.embedding_lookup(position_enc, position_ind) # (batch_size, maxlen1, d_model)\n",
    "\n",
    "            # masking，将pad的位置置为0\n",
    "            position_embeddings = tf.where(tf.equal(enc, 0), enc, position_embeddings)\n",
    "\n",
    "            position_embeddings = tf.to_float(position_embeddings) # 张量再转换为float32\n",
    "\n",
    "        enc += position_embeddings\n",
    "\n",
    "        # embedding dropout\n",
    "        enc = tf.layers.dropout(enc, dropout_rate, training=is_training) # (batch_size, maxlen1, d_model)\n",
    "\n",
    "        # encoder进行多头attention的映射\n",
    "        for i in range(num_blocks):\n",
    "            with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                with tf.variable_scope(\"multihead_attention\", reuse=tf.AUTO_REUSE):\n",
    "                    # Linear projections\n",
    "                    Q = tf.layers.dense(enc, d_model, use_bias=False) # (batch_size, maxlen1, d_model)\n",
    "                    K = tf.layers.dense(enc, d_model, use_bias=False)\n",
    "                    V = tf.layers.dense(enc, d_model, use_bias=False)\n",
    "\n",
    "                    # Split and concat\n",
    "                    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen1, d_model/num_heads)\n",
    "                    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen1, d_model/num_heads)\n",
    "                    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen1, d_model/num_heads)\n",
    "\n",
    "                    # Attention\n",
    "                    with tf.variable_scope(\"scaled_dot_product_attention\", reuse=tf.AUTO_REUSE):\n",
    "                        d_k = Q_.get_shape().as_list()[-1] # d_model/num_heads\n",
    "\n",
    "                        # dot product\n",
    "                        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (num_heads*batch_size, maxlen1, maxlen1)\n",
    "\n",
    "                        # scale\n",
    "                        outputs /= d_k ** 0.5\n",
    "\n",
    "                        # key masking, 把enc中padding的部分做mask\n",
    "                        masks = tf.sign(tf.reduce_sum(tf.abs(K_), axis=-1)) # (num_heads*batch_size, maxlen1)\n",
    "                        masks = tf.expand_dims(masks, 1) # (num_heads*batch_size, 1, maxlen1)\n",
    "                        masks = tf.tile(masks, [1, tf.shape(Q_)[1], 1]) # (num_heads*batch_size, maxlen1, maxlen1)\n",
    "                        paddings = tf.ones_like(outputs) * padding_num # (num_heads*batch_size, maxlen1, maxlen1)\n",
    "                        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (num_heads*batch_size, maxlen1, maxlen1)\n",
    "\n",
    "                        # softmax\n",
    "                        outputs = tf.nn.softmax(outputs)\n",
    "                        attention = tf.transpose(outputs, [0, 2, 1])\n",
    "                        tf.summary.image(\"attention\", tf.expand_dims(attention[:1], -1)) # 对key的softmax\n",
    "\n",
    "                        # query masking\n",
    "                        masks = tf.sign(tf.reduce_sum(tf.abs(Q_), axis=-1)) # (num_heads*batch_size, maxlen1)\n",
    "                        masks = tf.expand_dims(masks, -1) # (num_heads*batch_size, 1, maxlen1)\n",
    "                        masks = tf.tile(masks, [1, 1, tf.shape(K_)[1]]) # (num_heads*batch_size, maxlen1, maxlen1)\n",
    "                        outputs = outputs * masks\n",
    "\n",
    "                        # dropout\n",
    "                        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training)\n",
    "\n",
    "                        # weighted sum (context vectors)\n",
    "                        outputs = tf.matmul(outputs, V_) # (num_heads*batch_size, maxlen1, d_model/num_heads)\n",
    "\n",
    "                    # Restore shape\n",
    "                    # (batch_size, maxlen1, d_model)\n",
    "                    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)\n",
    "\n",
    "                    # Residual connection\n",
    "                    outputs += enc\n",
    "\n",
    "                    # Layer Normalize\n",
    "                    with tf.variable_scope(\"ln\", reuse=tf.AUTO_REUSE):\n",
    "                        outputs_shape = outputs.get_shape()\n",
    "                        params_shape = outputs_shape[-1:]\n",
    "\n",
    "                        mean, variance = tf.nn.moments(outputs, [-1], keep_dims=True)\n",
    "                        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "                        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "                        normalized = (outputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "                        outputs = gamma * normalized + beta\n",
    "\n",
    "\n",
    "                # feed forwards\n",
    "                with tf.variable_scope(\"positionwise_feedforward\", reuse=tf.AUTO_REUSE):\n",
    "                    # Inner layer\n",
    "                    outputs_ff = tf.layers.dense(outputs, d_ff, activation=tf.nn.relu)\n",
    "\n",
    "                    # Outer layer\n",
    "                    outputs_ff = tf.layers.dense(outputs, d_model)\n",
    "\n",
    "                    # Residual connection\n",
    "                    outputs_ff += outputs\n",
    "\n",
    "                    # Normalize\n",
    "                    with tf.variable_scope(\"ln\", reuse=tf.AUTO_REUSE):\n",
    "                        outputs_shape = outputs_ff.get_shape()\n",
    "                        params_shape = outputs_shape[-1:]\n",
    "\n",
    "                        mean, variance = tf.nn.moments(outputs_ff, [-1], keep_dims=True)\n",
    "                        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "                        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "                        normalized = (outputs_ff - mean) / ( (variance + epsilon) ** (.5) )\n",
    "                        outputs_ff = gamma * normalized + beta\n",
    "\n",
    "                enc = outputs_ff # (batch_size, maxlen1, d_model)\n",
    "            \n",
    "    memory = enc\n",
    "    return memory, sents1\n",
    "\n",
    "encoding_memory, sents1 = encoding(xs, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(ys_input, memory, is_training):\n",
    "    with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "        decoder_inputs, y, seqlens, sents2 = ys_input\n",
    "\n",
    "        # embedding\n",
    "        dec = tf.nn.embedding_lookup(embeddings, decoder_inputs) # (batch_size, maxlen2, d_model)\n",
    "        dec *= d_model ** 0.5  # scale\n",
    "\n",
    "        # 位置向量编码\n",
    "        E = dec.get_shape().as_list()[-1]\n",
    "        N, T = tf.shape(dec)[0], tf.shape(dec)[1]\n",
    "        with tf.variable_scope(\"positional_encoding\", reuse=tf.AUTO_REUSE):\n",
    "            position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # (batch_size, maxlen2)\n",
    "\n",
    "            position_enc = np.array([\n",
    "                [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n",
    "                for pos in range(maxlen2)])\n",
    "\n",
    "            position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])\n",
    "            position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])\n",
    "            position_enc = tf.convert_to_tensor(position_enc, tf.float32) # (maxlen2, d_model)\n",
    "\n",
    "            # lookup\n",
    "            position_embeddings = tf.nn.embedding_lookup(position_enc, position_ind) # (batch_size, maxlen2, d_model)\n",
    "\n",
    "            # masking\n",
    "            position_embeddings = tf.where(tf.equal(dec, 0), dec, position_embeddings)\n",
    "\n",
    "            position_embeddings = tf.to_float(position_embeddings)\n",
    "\n",
    "        dec += position_embeddings\n",
    "        dec = tf.layers.dropout(dec, dropout_rate, training=is_training)\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                # Masked self-attention\n",
    "                with tf.variable_scope(\"self_attention\", reuse=tf.AUTO_REUSE):\n",
    "                    Q = tf.layers.dense(dec, d_model, use_bias=False) # (batch_size, maxlen2, d_model)\n",
    "                    K = tf.layers.dense(dec, d_model, use_bias=False)\n",
    "                    V = tf.layers.dense(dec, d_model, use_bias=False)\n",
    "\n",
    "                    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen2, d_model/num_heads)\n",
    "                    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen2, d_model/num_heads)\n",
    "                    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen2, d_model/num_heads)\n",
    "\n",
    "                    with tf.variable_scope(\"scaled_dot_product_attention\", reuse=tf.AUTO_REUSE):\n",
    "                        d_k = Q_.get_shape().as_list()[-1] # d_model/num_heads\n",
    "\n",
    "                        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (num_heads*batch_size, maxlen2, maxlen2)\n",
    "\n",
    "                        outputs /= d_k ** 0.5\n",
    "\n",
    "                        masks = tf.sign(tf.reduce_sum(tf.abs(K_), axis=-1)) # (num_heads*batch_size, maxlen2)\n",
    "                        masks = tf.expand_dims(masks, 1) # (num_heads*batch_size, 1, maxlen2)\n",
    "                        masks = tf.tile(masks, [1, tf.shape(Q_)[1], 1]) # (num_heads*batch_size, maxlen2, maxlen2)\n",
    "                        paddings = tf.ones_like(outputs) * padding_num # (num_heads*batch_size, maxlen2, maxlen2)\n",
    "                        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (num_heads*batch_size, maxlen2, maxlen2)\n",
    "\n",
    "                        # 注意，这里和encoder不同，units that reference the future are masked\n",
    "                        diag_vals = tf.ones_like(outputs[0, :, :]) # (maxlen2, maxlen2)\n",
    "                        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (maxlen2, maxlen2) 上三角为0\n",
    "                        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])# (num_heads*batch_size, maxlen2, maxlen2)\n",
    "                        paddings = tf.ones_like(masks) * padding_num\n",
    "                        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n",
    "\n",
    "                        outputs = tf.nn.softmax(outputs)\n",
    "                        attention = tf.transpose(outputs, [0, 2, 1])\n",
    "                        tf.summary.image(\"attention\", tf.expand_dims(attention[:1], -1)) # 对key的softmax\n",
    "\n",
    "                        masks = tf.sign(tf.reduce_sum(tf.abs(Q_), axis=-1)) # (num_heads*batch_size, maxlen2)\n",
    "                        masks = tf.expand_dims(masks, -1) # (num_heads*batch_size, 1, maxlen2)\n",
    "                        masks = tf.tile(masks, [1, 1, tf.shape(K_)[1]]) # (num_heads*batch_size, maxlen1, maxlen2)\n",
    "                        outputs = outputs * masks\n",
    "\n",
    "                        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training)\n",
    "\n",
    "                        outputs = tf.matmul(outputs, V_) # (num_heads*batch_size, maxlen2, d_model/num_heads)\n",
    "\n",
    "                    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)\n",
    "\n",
    "                    outputs += dec\n",
    "\n",
    "                    with tf.variable_scope(\"ln\", reuse=tf.AUTO_REUSE):\n",
    "                        outputs_shape = outputs.get_shape()\n",
    "                        params_shape = outputs_shape[-1:]\n",
    "\n",
    "                        mean, variance = tf.nn.moments(outputs, [-1], keep_dims=True)\n",
    "                        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "                        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "                        normalized = (outputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "                        outputs = gamma * normalized + beta\n",
    "\n",
    "                dec = outputs\n",
    "\n",
    "                # Vanilla attention\n",
    "                with tf.variable_scope(\"vanilla_attention\", reuse=tf.AUTO_REUSE):\n",
    "                    # Linear projections\n",
    "                    Q = tf.layers.dense(dec, d_model, use_bias=False) # (batch_size, maxlen2, d_model)\n",
    "                    K = tf.layers.dense(memory, d_model, use_bias=False) # (batch_size, maxlen1, d_model)\n",
    "                    V = tf.layers.dense(memory, d_model, use_bias=False) # (batch_size, maxlen1, d_model)\n",
    "\n",
    "                    # Split and concat\n",
    "                    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen2, d_model/num_heads)\n",
    "                    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen1, d_model/num_heads)\n",
    "                    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (num_heads*batch_size, maxlen1, d_model/num_heads)\n",
    "\n",
    "                    # Attention\n",
    "                    with tf.variable_scope(\"scaled_dot_product_attention\", reuse=tf.AUTO_REUSE):\n",
    "                        d_k = Q_.get_shape().as_list()[-1]\n",
    "\n",
    "                        # dot product\n",
    "                        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (num_heads*batch_size, maxlen2, maxlen1)\n",
    "\n",
    "                        # scale\n",
    "                        outputs /= d_k ** 0.5\n",
    "\n",
    "                        # key masking\n",
    "                        masks = tf.sign(tf.reduce_sum(tf.abs(K_), axis=-1)) # (num_heads*batch_size, maxlen1)\n",
    "                        masks = tf.expand_dims(masks, 1) # (num_heads*batch_size, 1, maxlen1)\n",
    "                        masks = tf.tile(masks, [1, tf.shape(Q_)[1], 1]) # (num_heads*batch_size, maxlen2, maxlen1)\n",
    "                        paddings = tf.ones_like(outputs) * padding_num\n",
    "                        outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (num_heads*batch_size, maxlen2, maxlen1)\n",
    "\n",
    "                        # softmax\n",
    "                        outputs = tf.nn.softmax(outputs)\n",
    "                        attention = tf.transpose(outputs, [0, 2, 1])\n",
    "                        tf.summary.image(\"attention\", tf.expand_dims(attention[:1], -1))\n",
    "\n",
    "                        # query masking\n",
    "                        masks = tf.sign(tf.reduce_sum(tf.abs(Q_), axis=-1))  # (num_heads*batch_size, maxlen2)\n",
    "                        masks = tf.expand_dims(masks, -1)\n",
    "                        masks = tf.tile(masks, [1, 1, tf.shape(K_)[1]])  # (num_heads*batch_size, maxlen1, maxlen2)\n",
    "                        outputs = outputs * masks\n",
    "\n",
    "                        # dropout\n",
    "                        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training)\n",
    "\n",
    "                        # weighted sum (context vectors)\n",
    "                        outputs = tf.matmul(outputs, V_)  # (num_heads*batch_size, maxlen2, d_model)\n",
    "\n",
    "                    # Restore shape\n",
    "                    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (num_heads*batch_size, maxlen2, d_model)\n",
    "\n",
    "                    # Residual connection\n",
    "                    outputs += dec\n",
    "\n",
    "                    # Normalize\n",
    "                    with tf.variable_scope(\"ln\", reuse=tf.AUTO_REUSE):\n",
    "                        outputs_shape = outputs.get_shape()\n",
    "                        params_shape = outputs_shape[-1:]\n",
    "\n",
    "                        mean, variance = tf.nn.moments(outputs, [-1], keep_dims=True)\n",
    "                        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "                        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "                        normalized = (outputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "                        outputs = gamma * normalized + beta\n",
    "\n",
    "                # feed forwards\n",
    "                with tf.variable_scope(\"positionwise_feedforward\", reuse=tf.AUTO_REUSE):\n",
    "                    # Inner layer\n",
    "                    outputs_ff = tf.layers.dense(outputs, d_ff, activation=tf.nn.relu)\n",
    "\n",
    "                    # Outer layer\n",
    "                    outputs_ff = tf.layers.dense(outputs, d_model)\n",
    "\n",
    "                    # Residual connection\n",
    "                    outputs_ff += outputs\n",
    "\n",
    "                    # Normalize\n",
    "                    with tf.variable_scope(\"ln\", reuse=tf.AUTO_REUSE):\n",
    "                        outputs_shape = outputs_ff.get_shape()\n",
    "                        params_shape = outputs_shape[-1:]\n",
    "\n",
    "                        mean, variance = tf.nn.moments(outputs_ff, [-1], keep_dims=True)\n",
    "                        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "                        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "                        normalized = (outputs_ff - mean) / ( (variance + epsilon) ** (.5) )\n",
    "                        outputs_ff = gamma * normalized + beta\n",
    "\n",
    "                dec = outputs_ff # (batch_size, maxlen2, d_model)\n",
    "\n",
    "    # Final linear projection (embedding weights are shared)\n",
    "    weights = tf.transpose(embeddings) # (d_model, vocab_size)\n",
    "    logits = tf.einsum('ntd,dk->ntk', dec, weights) # (batch_size, maxlen2, d_model)\n",
    "    preds = tf.to_int32(tf.argmax(logits, axis=-1))   \n",
    "    \n",
    "    return logits, preds, y, sents2\n",
    "\n",
    "logits, preds, y, sents2 = decoding(ys, encoding_memory, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义train scheme\n",
    "\n",
    "label smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_inputs = tf.one_hot(y, depth=vocab_size)\n",
    "label_V = label_inputs.get_shape().as_list()[-1]\n",
    "y_ = ((1 - epsilon) * label_inputs) + (epsilon / label_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)\n",
    "nonpadding = tf.to_float(tf.not_equal(y, token2idx[\"<pad>\"]))  # 0: <pad>\n",
    "loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.0003\n",
    "warmup_steps = 4000\n",
    "\n",
    "def noam_scheme(init_lr, global_step, warmup_steps=4000.):\n",
    "    '''Noam scheme learning rate decay\n",
    "    init_lr: initial learning rate. scalar.\n",
    "    global_step: scalar.\n",
    "    warmup_steps: scalar. During warmup_steps, learning rate increases\n",
    "        until it reaches init_lr.\n",
    "    '''\n",
    "    step = tf.cast(global_step + 1, dtype=tf.float32)\n",
    "    return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "lr = noam_scheme(lr, global_step, warmup_steps)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar('lr', lr)\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "tf.summary.scalar(\"global_step\", global_step)\n",
    "\n",
    "train_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义评测和推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_idx_to_token_tensor(inputs, idx2token):\n",
    "    '''Converts int32 tensor to string tensor.\n",
    "    inputs: 1d int32 tensor. indices.\n",
    "    idx2token: dictionary\n",
    "\n",
    "    Returns\n",
    "    1d string tensor.\n",
    "    '''\n",
    "    def my_func(inputs):\n",
    "        return \" \".join(idx2token[elem] for elem in inputs)\n",
    "\n",
    "    return tf.py_func(my_func, [inputs], tf.string)\n",
    "\n",
    "def evaluation(xs, ys):\n",
    "    decoder_inputs, y, y_seqlen, sents2 = ys\n",
    "    decoder_inputs = tf.ones((tf.shape(xs[0])[0], 1), tf.int32) * token2idx[\"<s>\"]\n",
    "    ys_decoder = (decoder_inputs, y, y_seqlen, sents2)\n",
    "    \n",
    "    memory, sents1 = encoding(xs, False)\n",
    "    logging.info(\"Inference graph is being built. Please be patient.\")\n",
    "    for _ in tqdm(range(maxlen2)):\n",
    "        logits, y_hat, y, sents2 = decoding(ys, memory, False)\n",
    "        if tf.reduce_sum(y_hat, 1) == token2idx[\"<pad>\"]: break\n",
    "\n",
    "        _decoder_inputs = tf.concat((decoder_inputs, y_hat), 1)\n",
    "        ys = (_decoder_inputs, y, y_seqlen, sents2)\n",
    "        \n",
    "    # monitor a random sample\n",
    "    n = tf.random_uniform((), 0, tf.shape(y_hat)[0]-1, tf.int32)\n",
    "    sent1 = sents1[n]\n",
    "    pred = convert_idx_to_token_tensor(y_hat[n], idx2token)\n",
    "    sent2 = sents2[n]\n",
    "    \n",
    "    tf.summary.text(\"sent1\", sent1)\n",
    "    tf.summary.text(\"pred\", pred)\n",
    "    tf.summary.text(\"sent2\", sent2)\n",
    "    summaries = tf.summary.merge_all()\n",
    "    \n",
    "    return y_hat, summaries\n",
    "\n",
    "y_hat, eval_summaries = evaluation(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "logdir = 'log'\n",
    "evaldir = 'eval'\n",
    "\n",
    "def save_variable_specs(fpath):\n",
    "    '''Saves information about variables such as\n",
    "    their name, shape, and total parameter number\n",
    "    fpath: string. output file path\n",
    "\n",
    "    Writes\n",
    "    a text file named fpath.\n",
    "    '''\n",
    "    def _get_size(shp):\n",
    "        '''Gets size of tensor shape\n",
    "        shp: TensorShape\n",
    "\n",
    "        Returns\n",
    "        size\n",
    "        '''\n",
    "        size = 1\n",
    "        for d in range(len(shp)):\n",
    "            size *=shp[d]\n",
    "        return size\n",
    "\n",
    "    params, num_params = [], 0\n",
    "    for v in tf.global_variables():\n",
    "        params.append(\"{}==={}\".format(v.name, v.shape))\n",
    "        num_params += _get_size(v.shape)\n",
    "    print(\"num_params: \", num_params)\n",
    "    with open(fpath, 'w') as fout:\n",
    "        fout.write(\"num_params: {}\\n\".format(num_params))\n",
    "        fout.write(\"\\n\".join(params))\n",
    "    logging.info(\"Variables info has been saved.\")\n",
    "    \n",
    "logging.info(\"# Session\")\n",
    "saver = tf.train.Saver(max_to_keep=num_epochs)\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.latest_checkpoint(logdir)\n",
    "    if ckpt is None:\n",
    "        logging.info(\"Initializing from scratch\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        save_variable_specs(os.path.join(logdir, \"specs\"))\n",
    "    else:\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "    sess.run(train_init_op)\n",
    "    total_steps = num_epochs * num_train_batches\n",
    "    _gs = sess.run(global_step)\n",
    "    for i in tqdm(range(_gs, total_steps+1)):\n",
    "        _, _gs, _summary = sess.run([train_op, global_step, train_summaries])\n",
    "        epoch = math.ceil(_gs / num_train_batches)\n",
    "        summary_writer.add_summary(_summary, _gs)\n",
    "\n",
    "        if _gs and _gs % num_train_batches == 0:\n",
    "            logging.info(\"epoch {} is done\".format(epoch))\n",
    "            _loss = sess.run(loss) # train loss\n",
    "\n",
    "            logging.info(\"# test evaluation\")\n",
    "            _, _eval_summaries = sess.run([eval_init_op, eval_summaries])\n",
    "            summary_writer.add_summary(_eval_summaries, _gs)\n",
    "\n",
    "            logging.info(\"# get hypotheses\")\n",
    "            hypotheses = get_hypotheses(num_eval_batches, num_eval_samples, sess, y_hat, idx2token)\n",
    "\n",
    "            logging.info(\"# write results\")\n",
    "            model_output = \"en2zh_E%02dL%.2f\" % (epoch, _loss)\n",
    "            if not os.path.exists(evaldir): os.makedirs(evaldir)\n",
    "            translation = os.path.join(evaldir, model_output)\n",
    "            with open(translation, 'w') as fout:\n",
    "                fout.write(\"\\n\".join(hypotheses))\n",
    "\n",
    "            logging.info(\"# calc bleu score and append it to translation\")\n",
    "            calc_bleu(hp.eval3, translation)\n",
    "\n",
    "            logging.info(\"# save models\")\n",
    "            ckpt_name = os.path.join(logdir, model_output)\n",
    "            saver.save(sess, ckpt_name, global_step=_gs)\n",
    "            logging.info(\"after training of {} epochs, {} has been saved.\".format(epoch, ckpt_name))\n",
    "\n",
    "            logging.info(\"# fall back to train mode\")\n",
    "            sess.run(train_init_op)\n",
    "    summary_writer.close()\n",
    "\n",
    "logging.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda-transformer)",
   "language": "python",
   "name": "conda-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
